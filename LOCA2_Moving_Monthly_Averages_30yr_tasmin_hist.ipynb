{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a671d21-2480-4b9b-9be1-c31f94ee912a",
   "metadata": {},
   "source": [
    "# LOCA2 30-year Moving Mean Annual Min Temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abf373a4-9794-4ddf-a5aa-aafc395e58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "#\n",
    "# Library Calls.\n",
    "#\n",
    "\n",
    "# loading numpy\n",
    "\n",
    "import numpy             as np\n",
    "\n",
    "# loading matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# loading xarray\n",
    "\n",
    "import xarray            as xr\n",
    "\n",
    "import datetime as datetime\n",
    "\n",
    "# Loading pandas\n",
    "\n",
    "import pandas            as pd\n",
    "\n",
    "import os as os\n",
    "\n",
    "import subprocess as subprocess\n",
    "\n",
    "def geo_idx(dd, dd_array):\n",
    "\n",
    "    geo_idx = (np.abs(dd_array - dd)).argmin()\n",
    "    return geo_idx\n",
    " \n",
    "#\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad579283-7a18-48d0-ae9f-68d6444a0b3e",
   "metadata": {},
   "source": [
    "##  File Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2f22db-7abf-446e-b510-dec12f17ee97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8993baf1-e898-4c48-8b10-e10dd4ec1a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "#\n",
    "# File Control\n",
    "#\n",
    "\n",
    "Original_File_Prefix = \"LOCA2-CONUS-MONTHLY_MEAN\"\n",
    "Final_File_Prefix    = \"LOCA2-CONUS-ANNUAL30YRUNMEAN_MONTHLYMEAN\"\n",
    "\n",
    "variable    = \"tasmin\"\n",
    "\n",
    "tempfile    = \"./\" + variable + \"_tempfile.nc\"\n",
    "memberfile  = \"./\" + variable + \"_model_member.nc\"\n",
    "\n",
    "local_hdf_string = \"export HDF5_USE_FILE_LOCKING=FALSE && \"\n",
    "local_hdf_string = \" \"\n",
    "\n",
    "cell_method    = \"time: minimum within days  time: mean within months  time: mean over 30 years \"\n",
    "cell_methodsdv = \"time: mean over months   time: stdev over 30 years \"\n",
    "\n",
    "target_rank =  \"1\"\n",
    "rank00      = \"01\"\n",
    "\n",
    "root_directory        = \"/data/DATASETS/LOCA_MACA_Ensembles/LOCA2/LOCA2_CONUS/Climate_CONUS/Monthly\"\n",
    "root_url              = \"http://kyrill.ias.sdsmt.edu:8080/thredds/dodsC/LOCA2/Climate_CONUS/Monthly\"\n",
    "\n",
    "loca2_inventory_file  = \"/data/DATASETS/LOCA_MACA_Ensembles/LOCA2/LOCA2_CONUS/LOCA2_Model_Member_Available_List.csv\"\n",
    "loca2_complete_file   = \"/data/DATASETS/LOCA_MACA_Ensembles/LOCA2/LOCA2_CONUS/LOCA2_Model_Member_Complete_List.csv\"\n",
    "\n",
    "loca2_mask            = \"/data/DATASETS/LOCA_MACA_Ensembles/LOCA2/LOCA2_CONUS/LOCA2_MASKS.nc\"\n",
    "\n",
    "#\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f970dc89-b432-427f-bf42-24162c298f93",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/DATASETS/LOCA_MACA_Ensembles/LOCA2/LOCA2_CONUS/LOCA2_MASKS.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/xarray/backends/file_manager.py:219\u001b[39m, in \u001b[36mCachingFileManager._acquire_with_cache_info\u001b[39m\u001b[34m(self, needs_lock)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     file = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/xarray/backends/lru_cache.py:56\u001b[39m, in \u001b[36mLRUCache.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28mself\u001b[39m._cache.move_to_end(key)\n",
      "\u001b[31mKeyError\u001b[39m: [<class 'netCDF4._netCDF4.Dataset'>, ('/data/DATASETS/LOCA_MACA_Ensembles/LOCA2/LOCA2_CONUS/LOCA2_MASKS.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), 'db6aa69f-f8df-4727-869e-3852119d86b6']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     14\u001b[39m yearall = xr.DataArray(name = \u001b[33m\"\u001b[39m\u001b[33myear\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m                       data   = years_middle,\n\u001b[32m     16\u001b[39m                       dims   = {\u001b[33m\"\u001b[39m\u001b[33myear\u001b[39m\u001b[33m\"\u001b[39m: n_runnings},\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m                                 \u001b[33m\"\u001b[39m\u001b[33mlong_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmiddle calendar year for 30-yr period\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     20\u001b[39m                                 \u001b[33m\"\u001b[39m\u001b[33mbounds\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33myear_bnds\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m     23\u001b[39m year_bnds = xr.DataArray(name = \u001b[33m\"\u001b[39m\u001b[33myear_bnds\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     24\u001b[39m                       data   =  years_bounds,\n\u001b[32m     25\u001b[39m                       coords = {\u001b[33m\"\u001b[39m\u001b[33myear\u001b[39m\u001b[33m\"\u001b[39m: years_middle,\n\u001b[32m     26\u001b[39m                                 \u001b[33m\"\u001b[39m\u001b[33mbnds\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m2\u001b[39m},\n\u001b[32m     27\u001b[39m                       attrs  = {\u001b[33m\"\u001b[39m\u001b[33mdescription\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mboundary calendar years for 30-yr period\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     28\u001b[39m                                 \u001b[33m\"\u001b[39m\u001b[33mlong_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mboundary calendar years for 30-yr period\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m ds_masks = \u001b[43mxr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mloca2_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m lon_bnds = ds_masks[\u001b[33m\"\u001b[39m\u001b[33mlon_bounds\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     34\u001b[39m lat_bnds = ds_masks[\u001b[33m\"\u001b[39m\u001b[33mlat_bounds\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/xarray/backends/api.py:606\u001b[39m, in \u001b[36mopen_dataset\u001b[39m\u001b[34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, create_default_indexes, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m decoders = _resolve_decoders_kwargs(\n\u001b[32m    595\u001b[39m     decode_cf,\n\u001b[32m    596\u001b[39m     open_backend_dataset_parameters=backend.open_dataset_parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m    602\u001b[39m     decode_coords=decode_coords,\n\u001b[32m    603\u001b[39m )\n\u001b[32m    605\u001b[39m overwrite_encoded_chunks = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33moverwrite_encoded_chunks\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m606\u001b[39m backend_ds = \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    611\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    612\u001b[39m ds = _dataset_from_backend_dataset(\n\u001b[32m    613\u001b[39m     backend_ds,\n\u001b[32m    614\u001b[39m     filename_or_obj,\n\u001b[32m   (...)\u001b[39m\u001b[32m    625\u001b[39m     **kwargs,\n\u001b[32m    626\u001b[39m )\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:767\u001b[39m, in \u001b[36mNetCDF4BackendEntrypoint.open_dataset\u001b[39m\u001b[34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, auto_complex, lock, autoclose)\u001b[39m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mopen_dataset\u001b[39m(\n\u001b[32m    746\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    747\u001b[39m     filename_or_obj: T_PathFileOrDataStore,\n\u001b[32m   (...)\u001b[39m\u001b[32m    764\u001b[39m     autoclose=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    765\u001b[39m ) -> Dataset:\n\u001b[32m    766\u001b[39m     filename_or_obj = _normalize_path(filename_or_obj)\n\u001b[32m--> \u001b[39m\u001b[32m767\u001b[39m     store = \u001b[43mNetCDF4DataStore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    770\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    772\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclobber\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclobber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    773\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdiskless\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdiskless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    774\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpersist\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpersist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    775\u001b[39m \u001b[43m        \u001b[49m\u001b[43mauto_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauto_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    776\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m        \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautoclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    780\u001b[39m     store_entrypoint = StoreBackendEntrypoint()\n\u001b[32m    781\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:525\u001b[39m, in \u001b[36mNetCDF4DataStore.open\u001b[39m\u001b[34m(cls, filename, mode, format, group, clobber, diskless, persist, auto_complex, lock, lock_maker, autoclose)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    522\u001b[39m     manager = CachingFileManager(\n\u001b[32m    523\u001b[39m         netCDF4.Dataset, filename, mode=mode, kwargs=kwargs\n\u001b[32m    524\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautoclose\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:429\u001b[39m, in \u001b[36mNetCDF4DataStore.__init__\u001b[39m\u001b[34m(self, manager, group, mode, lock, autoclose)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28mself\u001b[39m._group = group\n\u001b[32m    428\u001b[39m \u001b[38;5;28mself\u001b[39m._mode = mode\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28mself\u001b[39m.format = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mds\u001b[49m.data_model\n\u001b[32m    430\u001b[39m \u001b[38;5;28mself\u001b[39m._filename = \u001b[38;5;28mself\u001b[39m.ds.filepath()\n\u001b[32m    431\u001b[39m \u001b[38;5;28mself\u001b[39m.is_remote = is_remote_uri(\u001b[38;5;28mself\u001b[39m._filename)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:534\u001b[39m, in \u001b[36mNetCDF4DataStore.ds\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:528\u001b[39m, in \u001b[36mNetCDF4DataStore._acquire\u001b[39m\u001b[34m(self, needs_lock)\u001b[39m\n\u001b[32m    527\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m        \u001b[49m\u001b[43mds\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_nc4_require_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/contextlib.py:137\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/xarray/backends/file_manager.py:207\u001b[39m, in \u001b[36mCachingFileManager.acquire_context\u001b[39m\u001b[34m(self, needs_lock)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;129m@contextmanager\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m) -> Iterator[T_File]:\n\u001b[32m    206\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     file, cached = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    209\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/xarray/backends/file_manager.py:225\u001b[39m, in \u001b[36mCachingFileManager._acquire_with_cache_info\u001b[39m\u001b[34m(self, needs_lock)\u001b[39m\n\u001b[32m    223\u001b[39m     kwargs = kwargs.copy()\n\u001b[32m    224\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mmode\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._mode\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m file = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_opener\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mode == \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;28mself\u001b[39m._mode = \u001b[33m\"\u001b[39m\u001b[33ma\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/netCDF4/_netCDF4.pyx:2517\u001b[39m, in \u001b[36mnetCDF4._netCDF4.Dataset.__init__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/netCDF4/_netCDF4.pyx:2154\u001b[39m, in \u001b[36mnetCDF4._netCDF4._ensure_nc_success\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/data/DATASETS/LOCA_MACA_Ensembles/LOCA2/LOCA2_CONUS/LOCA2_MASKS.nc'"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "#\n",
    "# Time Coordinates\n",
    "#\n",
    "\n",
    "years_start = np.arange(start =    1950, \n",
    "                        stop  =    1986,   dtype=np.float32)\n",
    "years_end   = np.array(years_start + 29, dtype=np.float32) \n",
    "\n",
    "years_middle = np.array(years_start/2 + years_end/2, dtype=np.float32)\n",
    "years_bounds = np.array([years_start,years_end]).transpose()\n",
    "n_runnings = len(years_start)\n",
    "\n",
    "yearall = xr.DataArray(name = \"year\",\n",
    "                      data   = years_middle,\n",
    "                      dims   = {\"year\": n_runnings},\n",
    "                      coords = {\"year\": years_middle},\n",
    "                      attrs  = {\"description\": \"middle calendar year for 30-yr period\",\n",
    "                                \"long_name\": \"middle calendar year for 30-yr period\",\n",
    "                                \"bounds\":\"year_bnds\"})\n",
    "\n",
    "\n",
    "year_bnds = xr.DataArray(name = \"year_bnds\",\n",
    "                      data   =  years_bounds,\n",
    "                      coords = {\"year\": years_middle,\n",
    "                                \"bnds\": 2},\n",
    "                      attrs  = {\"description\": \"boundary calendar years for 30-yr period\",\n",
    "                                \"long_name\": \"boundary calendar years for 30-yr period\"})\n",
    "\n",
    "\n",
    "ds_masks = xr.open_dataset(filename_or_obj = loca2_mask)\n",
    "\n",
    "lon_bnds = ds_masks[\"lon_bounds\"]\n",
    "lat_bnds = ds_masks[\"lat_bounds\"]\n",
    "lat_bnds.name = \"lat_bnds\"\n",
    "lon_bnds.name = \"lon_bnds\"\n",
    "\n",
    "lon = ds_masks[\"lon\"]\n",
    "lat = ds_masks[\"lat\"]\n",
    "\n",
    "\n",
    "print(lat)\n",
    "print(lon)\n",
    "print(year_bnds)\n",
    "\n",
    "#\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9f559d-a70a-4906-86b6-4499b507a222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "049a0503-94b1-4c5a-a0fe-c98ccbb56990",
   "metadata": {},
   "source": [
    "## Inventories and Lookup Tables\n",
    "### All Potential Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55407694-92d9-401f-9327-749aa795de09",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/DATASETS/LOCA_MACA_Ensembles/LOCA2/LOCA2_CONUS/LOCA2_Model_Member_Complete_List.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m##########################################################\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Inventory and Model/Member Lookup Table for all Possible Ensembles\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m loca2_complete_list = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mloca2_complete_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m modelsc               = loca2_complete_list[         \u001b[33m\"\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m\"\u001b[39m].values\n\u001b[32m     10\u001b[39m membersc              = loca2_complete_list[        \u001b[33m\"\u001b[39m\u001b[33mMember\u001b[39m\u001b[33m\"\u001b[39m].values\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/data/DATASETS/LOCA_MACA_Ensembles/LOCA2/LOCA2_CONUS/LOCA2_Model_Member_Complete_List.csv'"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "#\n",
    "# Inventory and Model/Member Lookup Table for all Possible Ensembles\n",
    "#\n",
    "\n",
    "\n",
    "loca2_complete_list = pd.read_csv(filepath_or_buffer = loca2_complete_file)\n",
    "\n",
    "modelsc               = loca2_complete_list[         \"Model\"].values\n",
    "membersc              = loca2_complete_list[        \"Member\"].values\n",
    "model_membersc        = loca2_complete_list[  \"Model_Member\"].values\n",
    "\n",
    "model_member_key0 = np.array(modelsc+\".\"+membersc, dtype=\"str\")\n",
    "model_member_key =  np.array([\"NULL\"], dtype=\"str\")\n",
    "\n",
    "model_members_to_save = np.append(0,model_membersc)\n",
    "\n",
    "model_member_key = np.append(model_member_key,model_member_key0)\n",
    "\n",
    "\n",
    "model_member_key    = xr.DataArray(model_member_key, \n",
    "                                   coords={\"model_member\":model_members_to_save},\n",
    "                                   name  = \"model_member_key\",\n",
    "                                   dims  = [\"model_member\"],\n",
    "                                   attrs = {\"description\" : \"Model and Member Label\",\n",
    "                                              \"long_name\" : \"Model and Member Label\",\n",
    "                                               \"comment1\" :   \"LUT Indexing Starts at 0\"})\n",
    "\n",
    "print(model_member_key)\n",
    "\n",
    "#\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840ffdc9-92ac-4e18-bc58-c870a0f50b84",
   "metadata": {},
   "source": [
    "### All Available Ensembles for Selected Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c046bd9b-7af6-45e6-a7bb-b4cfc3597444",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "#\n",
    "# Inventory and Model/Member Lookup Table for all Possible Ensembles\n",
    "#\n",
    "\n",
    "loca2_ensembles_list = pd.read_csv(filepath_or_buffer = loca2_inventory_file)\n",
    "\n",
    "loca2_ensembles_list = loca2_ensembles_list.query(\"Rank == \" + target_rank)\n",
    "\n",
    "models               = loca2_ensembles_list[         \"Model\"].values\n",
    "members              = loca2_ensembles_list[        \"Member\"].values\n",
    "model_members        = loca2_ensembles_list[  \"Model_Member\"].values\n",
    "n_complete_enss      = loca2_ensembles_list[\"n_complete_ens\"].values\n",
    "historical_invs      = loca2_ensembles_list[    \"historical\"].values\n",
    "ssp245_invs          = loca2_ensembles_list[        \"ssp245\"].values\n",
    "ssp370_invs          = loca2_ensembles_list[        \"ssp370\"].values\n",
    "ssp585_invs          = loca2_ensembles_list[        \"ssp585\"].values\n",
    "prec_invs            = loca2_ensembles_list[            \"pr\"].values\n",
    "tmax_invs            = loca2_ensembles_list[        \"tasmax\"].values\n",
    "tmin_invs            = loca2_ensembles_list[        \"tasmin\"].values\n",
    "\n",
    "scenarios            = [\"historical\", \n",
    "                            \"ssp245\", \n",
    "                            \"ssp370\", \n",
    "                            \"ssp585\"]\n",
    "\n",
    "print(loca2_ensembles_list)\n",
    "\n",
    "#\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c7b0ad-dc9a-4a7b-a3ab-1e42e618df93",
   "metadata": {},
   "source": [
    "## Loop for Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fcba28-84ee-4e83-b114-be1fcf13fe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "#\n",
    "# Loop Test\n",
    "#\n",
    "\n",
    "    \n",
    "for scenario in scenarios[0]:\n",
    "    print(\"# ################################################\")\n",
    "    print(\"# ################################################\")\n",
    "    print(\"# ################################################\")\n",
    "    First = True\n",
    "    for m in range(len(models)-1):\n",
    "        print(\"# ------------------------------------------------\")\n",
    "\n",
    "        model          =          models[m]\n",
    "        member         =         members[m]\n",
    "        model_member   =   model_members[m]\n",
    "\n",
    "        n_complete_ens = n_complete_enss[m]\n",
    "        prec_inv       =       prec_invs[m]\n",
    "        tmax_inv       =       tmax_invs[m]\n",
    "        tmin_inv       =       tmin_invs[m]\n",
    "\n",
    "        model_member_name = np.array([model + \".\" + member], dtype=\"str\")\n",
    "        model_member      = np.array([model_member], dtype=\"int16\").flatten().astype(np.int16)\n",
    "        \n",
    "\n",
    "\n",
    "        model_member = xr.DataArray(data   = model_member.astype(np.int16),\n",
    "                                    coords = {\"model_member\":model_member.astype(np.int16)},\n",
    "                                    name   =  \"model_member\",\n",
    "                                    dims   = [\"model_member\"],\n",
    "                                    attrs  = {\"description\" : \"Model and Member Code\",\n",
    "                                              \"long_name\"   : \"Model and Member Code\",\n",
    "                                              \"code_to_name_lookup_table\":  model_member_key.values.tolist(),\n",
    "                                              \"comment1\"    : \"LUT Indexing Starts at 0\"})\n",
    "\n",
    "        print(\"# ================================================\")\n",
    "\n",
    "        inventory = loca2_ensembles_list.iloc[m].loc[scenario]\n",
    "        print(\"# \" + str(model_member[0].values).zfill(3) + \" \" + model + \" \" + member + \" \" + scenario + \" \" + inventory)\n",
    "\n",
    "        if (inventory != \"---\"):\n",
    "\n",
    "\n",
    "            if (\"N\" in inventory):\n",
    "\n",
    "\n",
    "\n",
    "                print(\"#  . . . . . . . . . . . . . . . . . . . . . . . .\")\n",
    "\n",
    "                hist_file         = root_directory                            +  \"/\"  + \\\n",
    "                                    \"historical\"                              +  \"/\"  + \\\n",
    "                                    Original_File_Prefix                      + \"___\" + \\\n",
    "                                    variable                                  + \"___\" + \\\n",
    "                                    models[m]                                 +  \".\"  + \\\n",
    "                                    members[m]                                + \"___\" + \\\n",
    "                                    \"historical\"                              + \".nc\"  \n",
    "\n",
    "\n",
    "\n",
    "                combined_file     = root_directory                            +  \"/\"  + \\\n",
    "                                    Final_File_Prefix                         + \"___\" + \\\n",
    "                                    variable                                  + \"___\" + \\\n",
    "                                    str(model_member[0].values).zfill(3)      + \"___\" + \\\n",
    "                                    models[m]                                 +  \".\"  + \\\n",
    "                                    members[m]                                + \"___\" + \\\n",
    "                                    scenario                                  + \".nc\"  \n",
    " \n",
    "                combined_wc_files = root_directory                            +  \"/\"  + \\\n",
    "                                    Final_File_Prefix                         + \"___\" + \\\n",
    "                                    variable                                  + \"___\" + \\\n",
    "                                    \"???\"                                     + \"___\" + \\\n",
    "                                    \"*\"                                       + \"___\" + \\\n",
    "                                    scenario                                  + \".nc\" \n",
    "\n",
    "                final_merged_file = root_directory                            +  \"/\"  + \\\n",
    "                                    scenario                                  +  \"/\"  + \\\n",
    "                                    Final_File_Prefix                         + \"___\" + \\\n",
    "                                    variable                                  + \"___\" + \\\n",
    "                                    \"ALLRANK\"+ rank00                         + \"___\" + \\\n",
    "                                    scenario                                  + \".nc\" \n",
    "\n",
    "\n",
    "                print(\"    - hist_file: \" + hist_file)\n",
    "                print(\"    - comb_file: \" + combined_file)\n",
    "                print(\"    - comw_file: \" + combined_wc_files)\n",
    "                print(\"    - finl_file: \" + final_merged_file)\n",
    "                \n",
    "        \n",
    "                ds            = xr.open_dataset(filename_or_obj = hist_file)\n",
    "                time_hist_max = ds[\"time\"].values.max()\n",
    "                time_hist_n   = ds[variable].values.shape\n",
    "                \n",
    "                print(\"#    Max_Orig_Time = \" + str(time_hist_max) + \"      \" + str(time_hist_n) )\n",
    "                \n",
    "                if (First):\n",
    "                    model_member_array = np.array(model_members[m], dtype = \"int16\")\n",
    "                    First              = False\n",
    "                else:\n",
    "                    model_member_array = np.append(model_member_array, model_members[m]).flatten().astype(np.int16)\n",
    "                    \n",
    "                    \n",
    "\n",
    "                cdo_cat_command = \"cdo --no_history -f nc4 -z zip_9  mergetime \"\n",
    "\n",
    "                command_aggregate = \"cp -frv \" + hist_file + \" \" + tempfile\n",
    "                subprocess.run([\"rm -fr \" + tempfile],                             shell = True, check = True)\n",
    "                subprocess.run([command_aggregate],                                shell = True, check = True)\n",
    "                subprocess.run([\"ncatted -Oh -a bounds,lon,d,,      \" + tempfile], shell = True, check = True)\n",
    "                subprocess.run([\"ncatted -Oh -a bounds,lat,d,,      \" + tempfile], shell = True, check = True)\n",
    "                subprocess.run([\"ncatted -Oh -a _FillValue,lon,d,,  \" + tempfile], shell = True, check = True)\n",
    "                subprocess.run([\"ncatted -Oh -a _FillValue,lat,d,,  \" + tempfile], shell = True, check = True)\n",
    "                subprocess.run([\"ncatted -Oh -a _FillValue,time,d,, \" + tempfile], shell = True, check = True)\n",
    " \n",
    "                ds              = xr.open_dataset(filename_or_obj = tempfile)\n",
    "                tasmin0         = ds[variable]\n",
    "        \n",
    "                time_merged_max = ds[\"time\"].values.max()\n",
    "                time_merged_n   = ds[variable].values.shape\n",
    "                \n",
    "                print(\"#   Max_Merge_Time = \" + str(time_merged_max) + \"     \" + str(time_merged_n) )      \n",
    "        \n",
    "                subprocess.run([\"rm -fr \" + tempfile], shell = True, check = True)\n",
    "            \n",
    "            \n",
    "                \n",
    "                time = ds[\"time\"]\n",
    "                lon = ds[\"lon\"]\n",
    "                lat = ds[\"lat\"]\n",
    "                nt = time.shape[0]\n",
    "                nm = 12\n",
    "                ny = nt/12\n",
    "\n",
    "\n",
    "                start_year = time.dt.year.min().values\n",
    "                end_year   = time.dt.year.max().values\n",
    "                year  = np.arange(start = start_year,\n",
    "                                  stop  = end_year+1,\n",
    "                                  dtype = np.int16)\n",
    "                month = np.arange(start = 1,\n",
    "                                  stop  = 12+1,\n",
    "                                  dtype = np.int16)\n",
    "                \n",
    "                yeardv = xr.DataArray(name = \"year\",\n",
    "                                      data   =  year,\n",
    "                                    dims   = \"year\",\n",
    "                                    coords = {\"year\": year},\n",
    "                                    attrs  = {\"description\": \"beginning period calendar year\",\n",
    "                                                \"long_name\": \"beginning period calendar year\"})\n",
    "\n",
    "\n",
    "                \n",
    "                monthdv = xr.DataArray(data   =  month,\n",
    "                                      dims    = \"month\",\n",
    "                                      coords  = {\"month\": month},\n",
    "                                      attrs   = {\"description\": \"calendar month\",\n",
    "                                                   \"long_name\": \"calendar month\"})\n",
    "                \n",
    "                print(\"Start Reindexing\",os.system(\"date\"))\n",
    "                \n",
    "                multiindex_ds = ds.assign_coords(month = monthdv,\n",
    "                                                 year  = yeardv).  \\\n",
    "                                   stack(time2d=(\"year\",\n",
    "                                                 \"month\")).        \\\n",
    "                                   reset_index(\"time\", drop=True). \\\n",
    "                                   rename(time=\"time2d\").          \\\n",
    "                                   unstack(\"time2d\").drop_vars(\"time_bnds\")\n",
    "\n",
    "                print(\"Start Rolling Mean\",os.system(\"date\"))\n",
    "\n",
    "                \n",
    "                for i in range(n_runnings):\n",
    "                    if ((i % 10) == 0):\n",
    "                        print(\"   --- Processing \",years_start[i] , \"to\", years_end[i])\n",
    "                \n",
    "                    year_co = xr.DataArray(name = \"year\",\n",
    "                                           data   =np.array([years_start[i]/2+years_end[i]/2], dtype=np.float32),\n",
    "                                           dims   = \"year\",\n",
    "                                           coords = {\"year\": np.array([years_start[i]], dtype=np.int16)},\n",
    "                                           attrs  = {\"description\": \"beginning period calendar year\",\n",
    "                                                     \"long_name\": \"beginning period calendar year\"})\n",
    "                    \n",
    "                    if (i == 0):\n",
    "                        running_var = multiindex_ds[variable].sel(year = slice(years_start[i],years_end[i])).mean(dim=\"year\", keep_attrs = True).transpose(\"month\",\"lat\",\"lon\").expand_dims(dim={\"model_member\" : 1,\"year\":1}) \n",
    "                        running_var.coords[\"model_member\"]=model_member\n",
    "                        running_var.coords[\"year\"] = year_co\n",
    "                    else:\n",
    "                        temp_30 = multiindex_ds[variable].sel(year = slice(years_start[i],years_end[i])).mean(dim=\"year\").transpose(\"month\",\"lat\",\"lon\").expand_dims(dim={\"model_member\" : 1,\"year\":1}) \n",
    "                        temp_30.coords[\"model_member\"]=model_member\n",
    "                        temp_30.coords[\"year\"] = year_co\n",
    "                        running_var = xr.concat([running_var, temp_30], dim = \"year\")\n",
    "                        del temp_30\n",
    "                        \n",
    "                print(\"Finished Rolling Mean\",os.system(\"date\"))\n",
    "\n",
    "\n",
    "                outdata = xr.Dataset(data_vars = {\"model_member\" : model_member.astype(np.int16),\n",
    "                                                  \"year\"         : yearall,\n",
    "                                                #  \"year_bnds\"    : year_bnds,\n",
    "                                                  \"month\"        : monthdv,\n",
    "                                                  \"lat\"          : lat,\n",
    "                                                  #\"lat_bnds\"     : lat_bnds,\n",
    "                                                  \"lon\"          : lon,\n",
    "                                                  #\"lon_bnds\"     : lon_bnds,\n",
    "                                                   variable      : running_var},\n",
    "                                     attrs     = {\"scenario\"     : scenario})\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "                outdata.to_netcdf(path           =  combined_file, \n",
    "                                  mode           =            'w', \n",
    "                                  format         =      \"NETCDF4\",\n",
    "                                  engine         =     \"h5netcdf\", #\n",
    "                                  unlimited_dims = \"model_member\",\n",
    "                                  encoding       = {variable: {         \"zlib\":    True,\n",
    "                                                                  \"complevel\" :       7, \n",
    "                                                                       \"dtype\": \"int16\", \n",
    "                                                                \"scale_factor\":     0.1,\n",
    "                                                                  \"add_offset\":     0.0,                                        \n",
    "                                                                  \"_FillValue\":  -32767}})\n",
    "\n",
    "                print(\"Writing NetCDF Climate File\",os.system(\"date\"))\n",
    "                \n",
    "\n",
    "                             \n",
    "                \n",
    "            # end check on available variable\n",
    "        # end check on on available member\n",
    "            \n",
    "    #end loop on model\n",
    "\n",
    "    print(\"# = = = = = = = = = = = = = = = = = = = = = = = = \") \n",
    "    \n",
    "    model_member = xr.DataArray(data   = model_member_array.astype(np.int16),\n",
    "                                name   =  \"model_member\",\n",
    "                                dims   = [\"model_member\"],\n",
    "                                attrs  = {\"description\" : \"Model and Member Code\",\n",
    "                                          \"long_name\"   : \"Model and Member Code\",\n",
    "                                          \"code_to_name_lookup_table\":  model_member_key.values.tolist(),\n",
    "                                          \"comment1\"    : \"LUT Indexing Starts at 0\"})\n",
    "\n",
    "    model_member_ds = xr.Dataset(data_vars = {\"model_member\" : model_member})\n",
    "    \n",
    "    model_member_ds.to_netcdf(path            =   memberfile, \n",
    "                               mode           =           'w', \n",
    "                               format         =     \"NETCDF4\",\n",
    "                               engine         =    \"h5netcdf\", #\n",
    "                               unlimited_dims = \"model_member\")  \n",
    "        \n",
    "    cdo_cat_command = \" cdo --no_history -f nc4 -z zip_9 cat \"\n",
    "    nco_cat_command = \" ncrcat --4 --hst --dfl_lvl 9  \"\n",
    "    command_aggregate = nco_cat_command +combined_wc_files + \" \" + final_merged_file    \n",
    "    print(\"# Final Aggregation for \"+scenario)\n",
    "    subprocess.run([\"rm -fr \" + final_merged_file + \" \" + tempfile + \" 2_\" + tempfile], \n",
    "                   shell = True, \n",
    "                   check = True)    \n",
    "    subprocess.run([local_hdf_string + command_aggregate], \n",
    "                   shell = True, \n",
    "                   check = True)\n",
    "    print(\"# Files Concatenated\")\n",
    "    subprocess.run([\"rm -frv \" + combined_wc_files], \n",
    "                   shell = True, \n",
    "                   check = True)\n",
    "    print(\"# Files Cleaned\")\n",
    "\n",
    "\n",
    "# end loop on scenario\n",
    "                \n",
    "print(\"# ================================================\")\n",
    "print(\"end processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a897ee-2980-4f3c-ba78-06774e4bf0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0ff726-a1e5-40ad-9fe8-8053c821870e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747699fa-7f99-4db5-8b73-3c25f1efd84c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
